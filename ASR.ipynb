{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYaMoEDueuB4"
      },
      "outputs": [],
      "source": [
        "!pip install pydub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0BxUNqXME9s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OgAaXKpOu9q"
      },
      "outputs": [],
      "source": [
        "#import tensorflow_datasets as tfds\n",
        "#(train, val, test), data_info = tfds.load('speech_commands', split=['train', 'validation', 'test'], shuffle_files=True, as_supervised=False,\n",
        "#    with_info=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5aYqi2fphXR"
      },
      "outputs": [],
      "source": [
        "ds = tf.keras.utils.get_file(\n",
        "      'speech_commands_v0.02.tar.gz',\n",
        "      origin=\"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfpf75f1mJo_"
      },
      "outputs": [],
      "source": [
        "'''DATASET_PATH = 'data/mini_speech_commands'\n",
        "\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'mini_speech_commands.zip',\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQU_76Lrlr3G"
      },
      "outputs": [],
      "source": [
        "#!unzip './data/mini_speech_commands.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8KZJmmimO1N"
      },
      "outputs": [],
      "source": [
        "#commands = np.array(tf.io.gfile.listdir(str(DATASET_PATH)))\n",
        "#commands = commands[commands != 'README.md']\n",
        "#print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5z_g9M8lryG"
      },
      "outputs": [],
      "source": [
        "commands = np.array(tf.io.gfile.listdir(str('/content/data')))\n",
        "commands = commands[commands != 'README.md']\n",
        "commands = commands[commands != '.DS_Store']\n",
        "commands = commands[commands != 'testing_list.txt']\n",
        "commands = commands[commands != '.ipynb_checkpoints']\n",
        "commands = commands[commands != '_background_noise_']\n",
        "commands = commands[commands != 'LICENSE']\n",
        "commands = commands[commands != 'speech_commands_v0.02.tar.gz']\n",
        "commands = commands[commands != 'validation_list.txt']\n",
        "print('Commands:', commands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds908E11sTCj"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9QA-zyCsNKa"
      },
      "outputs": [],
      "source": [
        "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
        "filenames = tf.random.shuffle(filenames)\n",
        "num_samples = len(filenames)\n",
        "print('Number of total examples:', num_samples)\n",
        "\n",
        "print('Example file tensor:', filenames[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlY9e5M2sNHG"
      },
      "outputs": [],
      "source": [
        "train_files = filenames[:85000]\n",
        "val_files = filenames[85000: 85000 + 10000]\n",
        "test_files = filenames[85000 + 10000:]\n",
        "\n",
        "print('Training set size', len(train_files))\n",
        "print('Validation set size', len(val_files))\n",
        "print('Test set size', len(test_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfdCREk5soPC"
      },
      "outputs": [],
      "source": [
        "def decode_audio(audio_binary):\n",
        "  # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
        "  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
        "  try:\n",
        "      audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
        "  except:\n",
        "    pass\n",
        "  # Since all the data is single channel (mono), drop the `channels`\n",
        "  # axis from the array.\n",
        "  return tf.squeeze(audio, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GPX3-vZsoLg"
      },
      "outputs": [],
      "source": [
        "def get_label(file_path):\n",
        "  parts = tf.strings.split(\n",
        "      input=file_path,\n",
        "      sep=os.path.sep)\n",
        "  # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
        "  # to work in a TensorFlow graph.\n",
        "  return parts[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1boX-Rw_soJH"
      },
      "outputs": [],
      "source": [
        "def get_waveform_and_label(file_path):\n",
        "  label = get_label(file_path)\n",
        "  audio_binary = tf.io.read_file(file_path)\n",
        "  waveform = decode_audio(audio_binary)\n",
        "  return waveform, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW7DXUitswBi"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "\n",
        "waveform_ds = files_ds.map(\n",
        "    map_func=get_waveform_and_label,\n",
        "    num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--ZvwqzMsv-T"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
        "\n",
        "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  ax.plot(audio.numpy())\n",
        "  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  ax.set_title(label)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jjwc4yRssv7l"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Zero-padding for an audio waveform with less than 16,000 samples.\n",
        "  input_len = 16000\n",
        "  waveform = waveform[:input_len]\n",
        "  zero_padding = tf.zeros(\n",
        "      [16000] - tf.shape(waveform),\n",
        "      dtype=tf.float32)\n",
        "  # Cast the waveform tensors' dtype to float32.\n",
        "  waveform = tf.cast(waveform, dtype=tf.float32)\n",
        "  # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
        "  # clips are of the same length.\n",
        "  equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      equal_length, frame_length=255, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIqSBKiRtIim"
      },
      "outputs": [],
      "source": [
        "for waveform, label in waveform_ds.take(270):\n",
        "  label = label.numpy().decode('utf-8')\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "print('Label:', label)\n",
        "print('Waveform shape:', waveform.shape)\n",
        "print('Spectrogram shape:', spectrogram.shape)\n",
        "print('Audio playback')\n",
        "display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SvlRE2Ksv4s"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOP_g36ZtCNP"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "timescale = np.arange(waveform.shape[0])\n",
        "axes[0].plot(timescale, waveform.numpy())\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].set_xlim([0, 16000])\n",
        "\n",
        "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
        "axes[1].set_title('Spectrogram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkn4jUbJtCJ9"
      },
      "outputs": [],
      "source": [
        "def get_spectrogram_and_label_id(audio, label):\n",
        "  spectrogram = get_spectrogram(audio)\n",
        "  label_id = tf.argmax(label == commands)\n",
        "  return spectrogram, label_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36yCOZdUtCHN"
      },
      "outputs": [],
      "source": [
        "spectrogram_ds = waveform_ds.map(\n",
        "  map_func=get_spectrogram_and_label_id,\n",
        "  num_parallel_calls=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FOeKg8wtTFB"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "\n",
        "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
        "  r = i // cols\n",
        "  c = i % cols\n",
        "  ax = axes[r][c]\n",
        "  plot_spectrogram(spectrogram.numpy(), ax)\n",
        "  ax.set_title(commands[label_id.numpy()])\n",
        "  ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlUqjl2etTBA"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(files):\n",
        "  files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "  output_ds = files_ds.map(\n",
        "      map_func=get_waveform_and_label,\n",
        "      num_parallel_calls=AUTOTUNE)\n",
        "  output_ds = output_ds.map(\n",
        "      map_func=get_spectrogram_and_label_id,\n",
        "      num_parallel_calls=AUTOTUNE)\n",
        "  return output_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF1c5qOxtns5"
      },
      "outputs": [],
      "source": [
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUEsHHq_tnpZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPbzUN9ttnm7"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghxFMyYGuSfn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E62TF7fPtS-U"
      },
      "outputs": [],
      "source": [
        "\n",
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "  \n",
        "\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(224, 224),\n",
        "    # Normalize.\n",
        "    layers.Conv2D(96, 11, strides=4, padding='same'),\n",
        "    layers.Lambda(tf.nn.local_response_normalization),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(3, strides=2),\n",
        "    layers.Conv2D(256, 5, strides=4, padding='same'),\n",
        "    layers.Lambda(tf.nn.local_response_normalization),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(3, strides=2),\n",
        "    layers.Conv2D(384, 3, strides=4, padding='same'),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(384, 3, strides=4, padding='same'),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(256, 3, strides=4, padding='same'),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(4096, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(4096, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels)\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgNAbsJ3twxR"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-0wLhR3twtz"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB8dZOfdtwrq"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title('AlexNet')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksKgcOaktwox"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF6VLtK56uLU"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYARZEIO64gM"
      },
      "outputs": [],
      "source": [
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=commands,\n",
        "            yticklabels=commands,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnT_HOGV24Nf"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = model.predict(test_audio)\n",
        "y_true = test_labels\n",
        "\n",
        "cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "cce(y_true, y_pred).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iv-3Ilpaspz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, AveragePooling2D, Dense, Flatten\n",
        "import tensorflow.keras.activations as activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4P9pQr7-Gs0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8PVPBOp-UPD"
      },
      "outputs": [],
      "source": [
        "model_resnet=resnet50.ResNet50(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_shape=([224, 224, 1])\n",
        "    \n",
        ")\n",
        "\n",
        "model_2=models.Sequential()\n",
        "model_2.add(layers.Resizing(224,224))\n",
        "model_2.add(model_resnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ7OumA8a9J-"
      },
      "outputs": [],
      "source": [
        "lr, num_epochs, batch_size = 0.001, 10, 256\n",
        "model_2.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GozkV3Ya9Gh"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "history = model_2.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    #steps_per_epoch=50,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JofFf7K71nqs"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title('ResNet')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP3YiGzi1sUR"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YhAnKEP1w-B"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(model_2.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmnGo-8m213e"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = model_2.predict(test_audio)\n",
        "y_true = test_labels\n",
        "\n",
        "cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "cce(y_true, y_pred).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gejfWy6ylkY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import densenet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JYtfyfEyp4E"
      },
      "outputs": [],
      "source": [
        "model_densenet=densenet.DenseNet121(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_shape=([224, 224, 1])\n",
        "    \n",
        ")\n",
        "\n",
        "model_5=models.Sequential()\n",
        "model_5.add(layers.Resizing(224,224))\n",
        "model_5.add(model_densenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoIDoZUly4q4"
      },
      "outputs": [],
      "source": [
        "model_5.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA1uTg06zEUK"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "history = model_5.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    #steps_per_epoch=10,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a29ydgMnzEQy"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title('DenseNet')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = np.argmax(model_5.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cphnvRFp2xua"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = model_5.predict(test_audio)\n",
        "y_true = test_labels\n",
        "\n",
        "cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "cce(y_true, y_pred).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGZb0x_kGzvk"
      },
      "outputs": [],
      "source": [
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFoUgEnwNWXu"
      },
      "outputs": [],
      "source": [
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_3 = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(224, 224),\n",
        "    # Normalize.\n",
        "    #layers.Conv2D(96, 11, strides=4, padding='same'),\n",
        "    layers.DepthwiseConv2D( kernel_size=(11,11),   padding='same', depth_multiplier=5),\n",
        "    layers.Lambda(tf.nn.local_response_normalization),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(3, strides=2),\n",
        "    layers.DepthwiseConv2D( kernel_size=(3,3),   padding='same', depth_multiplier=5),\n",
        "    layers.Lambda(tf.nn.local_response_normalization),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(3, strides=2),\n",
        "    layers.DepthwiseConv2D( kernel_size=(3, 3), padding='same', depth_multiplier = 3),\n",
        "    layers.Activation('relu'),\n",
        "    #layers.Conv2D( 128, 3,  padding='same', strides=4),\n",
        "    layers.DepthwiseConv2D( kernel_size=(3,3),   padding='same', depth_multiplier=5),\n",
        "    layers.Activation('relu'),\n",
        "    layers.DepthwiseConv2D(3,depth_multiplier = 3, padding='same'),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(4096, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels)\n",
        "])\n",
        "\n",
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRp09Q7LGzsO"
      },
      "outputs": [],
      "source": [
        "model_3.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylf_W7NMGzpW"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model_3.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        "\n",
        "    \n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMGWKUqyzEp6"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title('DenseNet')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = np.argmax(model_3.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9zqVxE32m0H"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = model_3.predict(test_audio)\n",
        "y_true = test_labels\n",
        "\n",
        "cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "cce(y_true, y_pred).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BNIk-Bje_RK"
      },
      "outputs": [],
      "source": [
        "train_ds = spectrogram_ds\n",
        "val_ds = preprocess_dataset(val_files)\n",
        "test_ds = preprocess_dataset(test_files)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whZygvnLUDlN"
      },
      "outputs": [],
      "source": [
        "for spectrogram, _ in spectrogram_ds.take(1):\n",
        "  input_shape = spectrogram.shape\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(commands)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_4 = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(224, 224),\n",
        "    # Normalize.\n",
        "    layers.Conv2D(96, 11,strides=4,padding='same'),\n",
        "    layers.Lambda(tf.nn.local_response_normalization),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(3, strides=2),\n",
        "    layers.SeparableConv2D(64, 5,  padding='same'),\n",
        "    layers.Lambda(tf.nn.local_response_normalization),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPooling2D(3, strides=2),\n",
        "    layers.SeparableConv2D(128, 3,  padding='same', depth_multiplier=3),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Conv2D(64, 3, strides=4, padding='same'),\n",
        "    layers.Activation('relu'),\n",
        "    layers.SeparableConv2D(256, 3,  padding='same'),\n",
        "    layers.Activation('relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels)\n",
        "])\n",
        "\n",
        "model_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swOOgD-wU75M"
      },
      "outputs": [],
      "source": [
        "model_4.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcuDl5gcU-nS"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "history = model_4.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)\n",
        "    #steps_per_epoch=15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVaYRVG9zISd"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title('DenseNet')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = np.argmax(model_4.predict(test_audio), axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "test_acc = sum(y_pred == y_true) / len(y_true)\n",
        "print(f'Test set accuracy: {test_acc:.0%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DK25N8t1-gz"
      },
      "outputs": [],
      "source": [
        "test_audio = []\n",
        "test_labels = []\n",
        "\n",
        "for audio, label in test_ds:\n",
        "  test_audio.append(audio.numpy())\n",
        "  test_labels.append(label.numpy())\n",
        "\n",
        "test_audio = np.array(test_audio)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "\n",
        "y_pred = model_4.predict(test_audio)\n",
        "y_true = test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXN1WU-E1-bG"
      },
      "outputs": [],
      "source": [
        "cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "cce(y_true, y_pred).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L\n"
      ],
      "metadata": {
        "id": "1gzgusuODLFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ASR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}